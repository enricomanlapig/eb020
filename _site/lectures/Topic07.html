<!DOCTYPE html>
<html lang="" xml:lang="">
  <head>
    <title>Introducing OLS</title>
    <meta charset="utf-8" />
    <meta name="author" content="Enrico Manlapig" />
    <meta name="date" content="2022-03-04" />
    <script src="libs/header-attrs/header-attrs.js"></script>
    <link href="libs/remark-css/default.css" rel="stylesheet" />
    <link href="libs/remark-css/default-fonts.css" rel="stylesheet" />
    <script src="libs/js-cookie/js.cookie.js"></script>
    <script src="libs/peerjs/peerjs.min.js"></script>
    <script src="libs/tiny.toast/toast.min.js"></script>
    <link href="libs/xaringanExtra-broadcast/broadcast.css" rel="stylesheet" />
    <script src="libs/xaringanExtra-broadcast/broadcast.js"></script>
    <link href="libs/tachyons/tachyons.min.css" rel="stylesheet" />
    <link href="libs/panelset/panelset.css" rel="stylesheet" />
    <script src="libs/panelset/panelset.js"></script>
  </head>
  <body>
    <textarea id="source">


background-image: url("images/topic07/pexels-marta-longas-3143085.jpg")
background-position: right
background-size: 55%
background-color: #DBCB52

class: middle, left

.pull-left[
# Introducing OLS

### 

### Enrico Manlapig

### Westmont College

### `r Sys.Date()`
]




---
# What is regression?

Regression analysis is a set of techniques for estimating relationships between variables. Social scientists use regression analysis for at least three reasons:

1. Describe reality

1. Generate forecasts

1. Test hypotheses


???
Ultimately, this is a set of techniques for answer questions that you, the researcher, would like to bring to data. 


---

# Some examples

1. Demand

1. Keynesian consumption

1. Finance

1. Welfare

1. Production

???

1. DEMAND
`$$Q_{D}	=	f(P_{D}) =	\beta_{0}+\beta_{1}P_{D}$$`

Econometrics allows us to bring form to this theoretical relationship. It allows us to be explicit about the specific relationships that the theory has suggested.

Economic theory suggests that `\(\beta_{1}\)` should be negative. Econometrics allows us to answer how negative. 


2. Keynesian Consumption

`$$C=\beta_{0}+\beta_{1}Y^{D}$$`

Keynesian consumption theory tells us that consumption is chosen out of income. A fraction is required for survival and a fraction is consumed out of each dollar we earn.

In this case, `\(\beta_{1}\)` is the Marginal Propensity to Consume. It should be positive and between 0 and 1. `\(\beta_{0}\)` is autonomous consumption. It should be positive but less than `\(Y^{D}\)`.



3. Finance

$$
`\begin{align*}
(r_{A}-r_{f})	&amp;\propto	(r_{m}-r_{f}) \\
(r_{A}-r_{f})	&amp;=	\beta(r_{m}-r_{f}) \\
Y	&amp;=	\beta X
\end{align*}`
$$

Finance tells us that the return of an asset (e.g. stock) is proportional to the return on the market (S&amp;P500). Econometrics can tell us how sensitive the asset is to the market. In fact, this level of sensitivity is precisely one of the measures risk we talked about in our last topic - undiversifiable risk.



4. Welfare

$$
`\begin{align*}
\mbox{DWL}	&amp;\propto	t^{2} \\
\mbox{DWL}	&amp;=	\beta_{1}t^{2} \\
\mbox{DWL}	&amp;=	\beta_{1}X 
\end{align*}`
$$
where `\(X=t^{2}\)`

Theory tells us that the higher the tax we impose on our goods and services, the larger the inefficiency we introduce into the market. 


5. Production
$$
`\begin{align*}
Q	&amp;=	\alpha L^{\beta_{1}} \\
\ln Q	&amp;=	\beta_{0}+\beta_{1}\ln L \\
Y	&amp;=	\beta_{0}+\beta_{1}X
\end{align*}`
$$

where `\(\beta_{0}=\ln\alpha\)`, `\(Y=\ln Q\)`, and `\(X=\ln L\)`

Theory tells that the more labor we devote to production, the more output we should get.



Lots of areas in and business are approachable through regression analysis. We are going to focus on linear models. These relationships, although seemingly non-linear, can easily converted into linear models either by renaming variables or by performing simple transformations. See above.





---
# Different kinds of models

.panelset[
.panel[.panel-name[Economic Models]

1. Demand 
`$$\mathbb{E}(Q|P)=\beta_{0}+\beta_{1}P$$`

1. Finance
$$
`\begin{align*}
\mathbb{E} (r_a - r_f | r_m - r_f) &amp;= \beta (r_m - r_f) \\
\mathbb{E} (Y|X) &amp;= \beta X 
\end{align*}`
$$


]

.panel[.panel-name[Econometric Models]


1. Demand 
`$$Q=\beta_{0}+\beta_{1}P + \varepsilon$$`

1. Finance
$$
`\begin{align*}
r_{a}-r_{f}	&amp;=	\beta(r_{m}-r_{f}) + \varepsilon \\
R_a &amp;= \beta R_m + \varepsilon
\end{align*}`
$$


]


.panel[.panel-name[Estimated Models]

1. Demand 
$$
`\begin{align*}
\hat{Q} &amp;= \hat{\beta}_{0}+\hat{\beta}_{1}P \\
Q &amp;= \hat{\beta}_{0}+\hat{\beta}_{1}P + e \\
Q &amp;= \hat{Q} + \hat\varepsilon \\
Q &amp;= \hat{Q} + e
\end{align*}`
$$

1. Finance
$$
`\begin{align*}
\widehat{r_{a}-r_{f}}	&amp;=	\hat\beta(r_{m}-r_{f}) \\
\hat{R}_a &amp;= \hat{\beta} R_m + \hat{\varepsilon} \\
\hat{R}_a &amp;= \hat{\beta} R_m + e
\end{align*}`
$$


]

]




???

## Economic models

Economic models are abstractions from reality. They involve a set of assumptions that attempt to distill reality into a few key features.

### Demand

In this story, theory is telling us about the relationship between prices and quantities. Quantity, the left hand side variable, is called the dependent variable whereas price, the right hand side variable, is called the independent variable. 

This kind of language feels like we are asserting causality. Quantity depends on price. But this is not really the case. We need economic theory and common sense to make this leap from correlated to causal relationship - more on this later.

We call the `\(\betas\)` the coefficients of the model. 

* `\(\beta_{0}\)` is the intercept or constant term

* `\(\beta_{1}\)` is the slope coefficient. In a linear model, it tells us the average amount `\(Q\)` will increase by when `\(P\)` increases by one unit.



### Finance

Notice that the dependent variable is the risk premium - it's a function of two other variables: `\(r_a - r_f | r_m - r_f\)`

Do any of these represent reality? No! There are many things left out. Income, advertising, oil prices, inflation, interest rates etc. But the theory is silent about these. The world - and the data - contain this stuff so we will need a way to recognize their influence.


## Econometric models are a representation of reality. It decomposes variation in the dependent variable into variation explained by the economic regression model (also called the deterministic component) but also recognizes the importance of other (unobserved) factors called error. This error is assumed to be random or stochastic. 

The deterministic portion indicates that the value of `\(Y\)` is "determined" by a given value of `\(X\)` (which is assumed to be non-stochastic). It can be thought of as the expected value of `\(Y\)` given `\(X\)`‚Äînamely `\(E(Y|X)\)`‚Äîi.e. the mean (or average) value of the Ys associated with a particular value of X. This is also denoted the conditional expectation (that is, expectation of Y conditional on X).


### Demand

In this case, quantity demanded ($Q$) depends on a deterministic or structural component defined by theory ($\beta_{0}+\beta_{1}P$) and a stochastic component, given by `\(\varepsilon\)`. The `\(i\)` notation indicates individual observations. So `\(Q\)` and `\(P\)` vary across observations, whereas `\(\beta_{0}\)` and `\(\beta_{1}\)` are constant across observations.

In general, why might the stock return be different from what economic theory suggests for today? Perhaps sentiment? Having a bad day? Weather? News release? In this case, `\(t\)` represents time. 
---

# Where does error come from?

### Variables omitted from the model üôÄ

### Measurement error üòµ

### Model misspecification üëø

### Randomness üé≤

???

If any of these happen, the value observed in the market will differ from what the model predicts.

---
class: center, middle
# A bit more notation üòæ


### Cross section

$$
Q_i = \beta_0 + \beta_1 P_i + \varepsilon_i
$$

### Time series
$$
Q_t = \beta_0 + \beta_1 P_t + \varepsilon_t
$$

### Panel data
`$$Q_{i,t} = \beta_{0} + \beta_{1} P_{i,t} + \varepsilon_t$$`

---
# Your turn!

What kind of models are these? Name the different elements. 

1. `\(\mathbb{E}(Q^{S}|P)=\beta_{0}+\beta_{1}P\)`

2. `\(Y=\beta_{0}+\beta_{1}X+\varepsilon\)`

3. `\(\mbox{Sales}_{t}=\beta_{0}+\beta_{1}\mbox{Sales}_{t-1}+\varepsilon_{t}\)`

4. `\(\mbox{Unemployment}_{t}=\hat{\beta}_{0}+\hat{\beta}_{1}\mbox{Inflation}_{t}+e_{t}\)`

5. `\(\mbox{Attitude}_{it}=\beta_{0}+\beta_{1}\mbox{Activity}_{it}+\varepsilon_{it}\)`

6. `\(\mathbb{E}(Q|L)=\beta_{0}+\beta_{1}\ln L\)`

7. `\(\widehat{\text{Population}}_{t}=300+0.1_{1}t\)`

8. `\(\mbox{Height}_{i}=\beta_{0}+\beta_{1}\mbox{Weight}_{i}+\varepsilon_{i}\)`

9. `\(\widehat{\mbox{Bonus}}_{i}=\hat{\beta}_{0}+\hat{\beta}_{1}\mbox{Sales}_{i}\)`


---
class: middle, center

# Introducing Ordinary Least Squares


---
class: middle, center
# The Ordinary Least Squares algorithm estimates model parameters by minimizing **the sum of squared errors**



???

*Linear Regression* in general is a family of algorithms employed in supervised machine learning.

By parameters, we mean intercepts, slopes, errors etc


---
# What do you think?

What is the relationship between `bill_length_mm` and `body_mass_g`?

1. Load the `palmerpenguins`, `dplyr`, and `ggplot2` libraries
2. From the `palmerpenguins` dataset, visualize `bill_length_mm` and `body_mass_g` using a `geom_point` chart with `bill_length_mm` on the vertical axis
3. Based on your visualization, can you *guess* what `\(\hat\beta_0\)` and `\(\hat\beta_1\)` must be in this estimated model? 
`$$\text{bill_length_mm} = \hat\beta_0 + \hat\beta_1 \text{body_mass_g} + e$$`
4. Add `geom_abline(intercept = [your intercept guess], slope = [your slope guess], color = "red")` to plot your line to visualize your guess
5. Use `mutate()` to compute fitted values and errors and `summarise` to compute the sum of square errors


???

```r
library(palmerpenguins)

penguins %&gt;%
  select(bill_length_mm, body_mass_g) %&gt;%
  filter(complete.cases(.)) %&gt;%
  head()
```

```
## # A tibble: 6 x 2
##   bill_length_mm body_mass_g
##            &lt;dbl&gt;       &lt;int&gt;
## 1           39.1        3750
## 2           39.5        3800
## 3           40.3        3250
## 4           36.7        3450
## 5           39.3        3650
## 6           38.9        3625
```

```r
penguins %&gt;%
  select(bill_length_mm, body_mass_g) %&gt;%
  filter(complete.cases(.)) %&gt;%
  ggplot(aes(x = bill_length_mm,
             y = body_mass_g)) +
  geom_point() +
  ylim(-1000,6000) +
  theme_minimal() +
  geom_abline(aes(intercept = 3000, slope = 0.005), color = "red")
```

![](Topic07_files/figure-html/penguin_data-1.png)&lt;!-- --&gt;

---
class: center
# Variance decomposition

## Total Sum of Squares
`$$SST = \sum_i^n (\hat{Y} - \bar{Y})^2$$`

.pull-left[
## Sum of Squares Regression
`$$SSR = \sum_i^n (\hat{Y}_i - \bar{Y})^2$$`
]

.pull-right[
## Sum of Squares Error
`$$SSE = \sum_i^n (\hat{Y}_i - \bar{Y})^2$$`
]


???
Regression splits the variation of the dependent variable into explainable and unexplainable portions


```r
anova(lm(bill_length_mm ~ body_mass_g, data = penguins))
```

```
## Analysis of Variance Table
## 
## Response: bill_length_mm
##              Df Sum Sq Mean Sq F value    Pr(&gt;F)    
## body_mass_g   1 3599.7  3599.7  186.44 &lt; 2.2e-16 ***
## Residuals   340 6564.5    19.3                      
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
```

```r
sum(penguins$bill_depth_mm^2, na.rm = TRUE)
```

```
## [1] 101933.4
```



---
class: center

# Variance decomposition is powerful! üí™

## `$$SST = SSR + SSE$$`

.pull-left[
## Coefficient of Determination

`$$R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST}$$`
]


.pull-right[
## Standard Error
`$$\text{Standard Error} = s = \sqrt{\frac{SSE}{n-k}}$$` 
where `\(k\)` is the number of coefficients being estimated
]

---
class: center, middle

# OLS seeks to minimize `\(SSE\)`

For one variable...

$$
`\begin{align*}
\hat{\beta}_{1}	&amp;=\frac{\sum_{i=1}^{N}(X_{i}-\bar{X})(Y_{i}-\bar{Y})}{\sum_{i=1}^{N}(X_{i}-\bar{X})^{2}} 
= \frac{SSXY}{SSX} \\
\hat{\beta}_{0}	&amp;=\bar{Y}-\hat{\beta}_{1}\bar{X}
\end{align*}`
$$

---

# Running OLS regressions in R



```r
lm(mpg ~ disp, data = mtcars)
```

```
## 
## Call:
## lm(formula = mpg ~ disp, data = mtcars)
## 
## Coefficients:
## (Intercept)         disp  
##    29.59985     -0.04122
```




### ‚úÖ Try it on palmer penguins! 

Regress `bill_length_mm` against `body_mass_g` üêß 

???
OLS tries to minimize the sum of squared residuals of the vertical distance between the residuals (i.e. the estimated error terms) and the estimated regression line. This is attractive for at least three reasons:

* Relatively easy to use

* The goal of minimizing RSS is intuitively / theoretically appealing - we want the estimated regression equation to be as close as possible to the observed data

* OLS estimates have a number of useful characteristics




---
count: false
 
# Running OLS regressions in R
.panel1-lm_illustration-rotate[

```r
*head(mtcars)

car_reg &lt;- lm(mpg ~ disp,
              data = mtcars)
```
]
 
.panel2-lm_illustration-rotate[

```
##                    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
## Valiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1
```
]

---
count: false
 
# Running OLS regressions in R
.panel1-lm_illustration-rotate[

```r
car_reg &lt;- lm(mpg ~ disp,
              data = mtcars)
*car_reg
```
]
 
.panel2-lm_illustration-rotate[

```
## 
## Call:
## lm(formula = mpg ~ disp, data = mtcars)
## 
## Coefficients:
## (Intercept)         disp  
##    29.59985     -0.04122
```
]

---
count: false
 
# Running OLS regressions in R
.panel1-lm_illustration-rotate[

```r
car_reg &lt;- lm(mpg ~ disp,
              data = mtcars)

*summary(car_reg)
```
]
 
.panel2-lm_illustration-rotate[

```
## 
## Call:
## lm(formula = mpg ~ disp, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.8922 -2.2022 -0.9631  1.6272  7.2305 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 29.599855   1.229720  24.070  &lt; 2e-16 ***
## disp        -0.041215   0.004712  -8.747 9.38e-10 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.251 on 30 degrees of freedom
## Multiple R-squared:  0.7183,	Adjusted R-squared:  0.709 
## F-statistic: 76.51 on 1 and 30 DF,  p-value: 9.38e-10
```
]

&lt;style&gt;
.panel1-lm_illustration-rotate {
  color: black;
  width: 38.6060606060606%;
  hight: 32%;
  float: left;
  padding-left: 1%;
  font-size: 80%
}
.panel2-lm_illustration-rotate {
  color: black;
  width: 59.3939393939394%;
  hight: 32%;
  float: left;
  padding-left: 1%;
  font-size: 80%
}
.panel3-lm_illustration-rotate {
  color: black;
  width: NA%;
  hight: 33%;
  float: left;
  padding-left: 1%;
  font-size: 80%
}
&lt;/style&gt;




---

# Multiple linear regression


### `$$Y = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + \cdots + \beta_K X_K + \varepsilon$$`


### Some examples

1. `\(\mathbb{E}(\text{FinancialAid}|\text{SAT,Race,ParentIncome})=\beta_{0}+\beta_{1}\text{SAT}+\beta_{2}\text{Race}+\beta_{3}\text{ParentIncome}\)`

2. `\(\text{CarPrice}=\beta_{0}+\beta_{1}\text{EngineSize}+\beta_{2}\text{AdvertisingSpend}+\beta_{3}\text{Wheelbase}+\varepsilon\)`

3. `\(\widehat{\text{GDP}_t}=300+0.1_{1}t-0.9\text{Unemployment}-0.3\text{InterstRates}+0.1\text{PopulationGrowth}\)`

### ‚úÖ Try it on palmer penguins! üêß






???

The main The main difference between the single regression and multiple regression is the interpretation of the slope coefficients. 

A multiple regression coefficient indicates the change in the dependent variable associated with a one-unit increase in the independent variable holding the other independent variables constant. Note that omitted (and relevant) variables are therefore not held constant.

The intercept term, `\(\beta_0\)`, is the value of `\(Y\)` when all the `\(X\)`s and the error term equal zero Nevertheless, the underlying principle of minimizing the summed squared residuals remains the same.

---
count: false
 
# Running multiple regressions in R
.panel1-lm_mult_illustration-rotate[

```r
car_mult &lt;- lm(mpg ~ disp + hp,
              data = mtcars)
*car_mult
```
]
 
.panel2-lm_mult_illustration-rotate[

```
## 
## Call:
## lm(formula = mpg ~ disp + hp, data = mtcars)
## 
## Coefficients:
## (Intercept)         disp           hp  
##    30.73590     -0.03035     -0.02484
```
]

---
count: false
 
# Running multiple regressions in R
.panel1-lm_mult_illustration-rotate[

```r
car_mult &lt;- lm(mpg ~ disp + hp,
              data = mtcars)

*summary(car_mult)
```
]
 
.panel2-lm_mult_illustration-rotate[

```
## 
## Call:
## lm(formula = mpg ~ disp + hp, data = mtcars)
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -4.7945 -2.3036 -0.8246  1.8582  6.9363 
## 
## Coefficients:
##              Estimate Std. Error t value Pr(&gt;|t|)    
## (Intercept) 30.735904   1.331566  23.083  &lt; 2e-16 ***
## disp        -0.030346   0.007405  -4.098 0.000306 ***
## hp          -0.024840   0.013385  -1.856 0.073679 .  
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 3.127 on 29 degrees of freedom
## Multiple R-squared:  0.7482,	Adjusted R-squared:  0.7309 
## F-statistic: 43.09 on 2 and 29 DF,  p-value: 2.062e-09
```
]

&lt;style&gt;
.panel1-lm_mult_illustration-rotate {
  color: black;
  width: 38.6060606060606%;
  hight: 32%;
  float: left;
  padding-left: 1%;
  font-size: 80%
}
.panel2-lm_mult_illustration-rotate {
  color: black;
  width: 59.3939393939394%;
  hight: 32%;
  float: left;
  padding-left: 1%;
  font-size: 80%
}
.panel3-lm_mult_illustration-rotate {
  color: black;
  width: NA%;
  hight: 33%;
  float: left;
  padding-left: 1%;
  font-size: 80%
}
&lt;/style&gt;




---

# The `lm` object is bigger than it looks
.pull-left[
* Query the structure of the object with `str()`
* Extract the `coefficients`, the `residuals` and the `fitted.values`
* You can extract parameters from `summary(lm())` too! 
]

.pull-right[
![](https://media.giphy.com/media/GUhxglEk24PqTBPylw/giphy.gif)
]


---
# Remember `\(R^2\)`?

* `\(R^2\)` tells us how much variation the model can explain

    * `\(R^2 = \frac{SSR}{SST}\)`
    * Extract `\(R^2\)` from a `summary()` of the `lm()` object `summary(car_mult)$r.squared`
    
* `\(R^2\)` is popular because it is very convenient
    
    * `\(R^2\)` always takes values between 0 an 1
    * `\(R^2 = 1\)` means that all of the variation in the dependent variable is explained by the model
    * `\(R^2 = 0\)` means that none of the variation in the dependent variable is explained by the model

* But... `\(R^2\)` always increases with the number of variables in the model... üò¨

???

The problem is one of over-fitting.

---
count: false
 
# All of these have the same `\(R^2\)`
.panel1-datasaurus-rotate[

```r
datasaurus_dozen %&gt;%
* filter(dataset == "dots") %&gt;%
  ggplot(aes(x = x,
             y = y)) +
  geom_point() +
  geom_smooth(method = "lm",
              color = "red",
              se = FALSE) +
  theme_minimal()
```
]
 
.panel2-datasaurus-rotate[
![](Topic07_files/figure-html/datasaurus_rotate_01_output-1.png)&lt;!-- --&gt;
]

---
count: false
 
# All of these have the same `\(R^2\)`
.panel1-datasaurus-rotate[

```r
datasaurus_dozen %&gt;%
* filter(dataset == "v_lines") %&gt;%
  ggplot(aes(x = x,
             y = y)) +
  geom_point() +
  geom_smooth(method = "lm",
              color = "red",
              se = FALSE) +
  theme_minimal()
```
]
 
.panel2-datasaurus-rotate[
![](Topic07_files/figure-html/datasaurus_rotate_02_output-1.png)&lt;!-- --&gt;
]

---
count: false
 
# All of these have the same `\(R^2\)`
.panel1-datasaurus-rotate[

```r
datasaurus_dozen %&gt;%
* filter(dataset == "high_lines") %&gt;%
  ggplot(aes(x = x,
             y = y)) +
  geom_point() +
  geom_smooth(method = "lm",
              color = "red",
              se = FALSE) +
  theme_minimal()
```
]
 
.panel2-datasaurus-rotate[
![](Topic07_files/figure-html/datasaurus_rotate_03_output-1.png)&lt;!-- --&gt;
]

---
count: false
 
# All of these have the same `\(R^2\)`
.panel1-datasaurus-rotate[

```r
datasaurus_dozen %&gt;%
* filter(dataset == "slant_up") %&gt;%
  ggplot(aes(x = x,
             y = y)) +
  geom_point() +
  geom_smooth(method = "lm",
              color = "red",
              se = FALSE) +
  theme_minimal()
```
]
 
.panel2-datasaurus-rotate[
![](Topic07_files/figure-html/datasaurus_rotate_04_output-1.png)&lt;!-- --&gt;
]

---
count: false
 
# All of these have the same `\(R^2\)`
.panel1-datasaurus-rotate[

```r
datasaurus_dozen %&gt;%
* filter(dataset == "slant_down") %&gt;%
  ggplot(aes(x = x,
             y = y)) +
  geom_point() +
  geom_smooth(method = "lm",
              color = "red",
              se = FALSE) +
  theme_minimal()
```
]
 
.panel2-datasaurus-rotate[
![](Topic07_files/figure-html/datasaurus_rotate_05_output-1.png)&lt;!-- --&gt;
]

---
count: false
 
# All of these have the same `\(R^2\)`
.panel1-datasaurus-rotate[

```r
datasaurus_dozen %&gt;%
* filter(dataset == "x_shape") %&gt;%
  ggplot(aes(x = x,
             y = y)) +
  geom_point() +
  geom_smooth(method = "lm",
              color = "red",
              se = FALSE) +
  theme_minimal()
```
]
 
.panel2-datasaurus-rotate[
![](Topic07_files/figure-html/datasaurus_rotate_06_output-1.png)&lt;!-- --&gt;
]

---
count: false
 
# All of these have the same `\(R^2\)`
.panel1-datasaurus-rotate[

```r
datasaurus_dozen %&gt;%
* filter(dataset == "star") %&gt;%
  ggplot(aes(x = x,
             y = y)) +
  geom_point() +
  geom_smooth(method = "lm",
              color = "red",
              se = FALSE) +
  theme_minimal()
```
]
 
.panel2-datasaurus-rotate[
![](Topic07_files/figure-html/datasaurus_rotate_07_output-1.png)&lt;!-- --&gt;
]

---
count: false
 
# All of these have the same `\(R^2\)`
.panel1-datasaurus-rotate[

```r
datasaurus_dozen %&gt;%
* filter(dataset == "bullseye") %&gt;%
  ggplot(aes(x = x,
             y = y)) +
  geom_point() +
  geom_smooth(method = "lm",
              color = "red",
              se = FALSE) +
  theme_minimal()
```
]
 
.panel2-datasaurus-rotate[
![](Topic07_files/figure-html/datasaurus_rotate_08_output-1.png)&lt;!-- --&gt;
]

---
count: false
 
# All of these have the same `\(R^2\)`
.panel1-datasaurus-rotate[

```r
datasaurus_dozen %&gt;%
* filter(dataset == "dino") %&gt;%
  ggplot(aes(x = x,
             y = y)) +
  geom_point() +
  geom_smooth(method = "lm",
              color = "red",
              se = FALSE) +
  theme_minimal()
```
]
 
.panel2-datasaurus-rotate[
![](Topic07_files/figure-html/datasaurus_rotate_09_output-1.png)&lt;!-- --&gt;
]

&lt;style&gt;
.panel1-datasaurus-rotate {
  color: black;
  width: 38.6060606060606%;
  hight: 32%;
  float: left;
  padding-left: 1%;
  font-size: 80%
}
.panel2-datasaurus-rotate {
  color: black;
  width: 59.3939393939394%;
  hight: 32%;
  float: left;
  padding-left: 1%;
  font-size: 80%
}
.panel3-datasaurus-rotate {
  color: black;
  width: NA%;
  hight: 33%;
  float: left;
  padding-left: 1%;
  font-size: 80%
}
&lt;/style&gt;




???
All of these datasets has the same `\(R^2\)` 



---

# Alternatives to `\(R^2\)`

* Adjusted `\(R^2\)`: `$$\bar{R}^2 = \frac{n-1}{n-K-1}[R^2 - \frac{K}{n-1}]$$`

    * Output along with `\(R^2\)` with `summary()`
    
* Akaike Information Criterion (AIC)

    * `AIC()`
    
* Schwarz Bayseian Crition (BIC)

    * `BIC()`
    
### ‚úÖ Try them out on palmer penguins! üêß




---
Have a look at `str(car_mult)`


https://pudding.cool/2018/08/pockets/






    </textarea>
<style data-target="print-only">@media screen {.remark-slide-container{display:block;}.remark-slide-scaler{box-shadow:none;}}</style>
<script src="https://remarkjs.com/downloads/remark-latest.min.js"></script>
<script>var slideshow = remark.create({
"highlightStyle": "github",
"highlightLines": true,
"countIncrementalSlides": false,
"ratio": "16:9"
});
if (window.HTMLWidgets) slideshow.on('afterShowSlide', function (slide) {
  window.dispatchEvent(new Event('resize'));
});
(function(d) {
  var s = d.createElement("style"), r = d.querySelector(".remark-slide-scaler");
  if (!r) return;
  s.type = "text/css"; s.innerHTML = "@page {size: " + r.style.width + " " + r.style.height +"; }";
  d.head.appendChild(s);
})(document);

(function(d) {
  var el = d.getElementsByClassName("remark-slides-area");
  if (!el) return;
  var slide, slides = slideshow.getSlides(), els = el[0].children;
  for (var i = 1; i < slides.length; i++) {
    slide = slides[i];
    if (slide.properties.continued === "true" || slide.properties.count === "false") {
      els[i - 1].className += ' has-continuation';
    }
  }
  var s = d.createElement("style");
  s.type = "text/css"; s.innerHTML = "@media print { .has-continuation { display: none; } }";
  d.head.appendChild(s);
})(document);
// delete the temporary CSS (for displaying all slides initially) when the user
// starts to view slides
(function() {
  var deleted = false;
  slideshow.on('beforeShowSlide', function(slide) {
    if (deleted) return;
    var sheets = document.styleSheets, node;
    for (var i = 0; i < sheets.length; i++) {
      node = sheets[i].ownerNode;
      if (node.dataset["target"] !== "print-only") continue;
      node.parentNode.removeChild(node);
    }
    deleted = true;
  });
})();
(function() {
  "use strict"
  // Replace <script> tags in slides area to make them executable
  var scripts = document.querySelectorAll(
    '.remark-slides-area .remark-slide-container script'
  );
  if (!scripts.length) return;
  for (var i = 0; i < scripts.length; i++) {
    var s = document.createElement('script');
    var code = document.createTextNode(scripts[i].textContent);
    s.appendChild(code);
    var scriptAttrs = scripts[i].attributes;
    for (var j = 0; j < scriptAttrs.length; j++) {
      s.setAttribute(scriptAttrs[j].name, scriptAttrs[j].value);
    }
    scripts[i].parentElement.replaceChild(s, scripts[i]);
  }
})();
(function() {
  var links = document.getElementsByTagName('a');
  for (var i = 0; i < links.length; i++) {
    if (/^(https?:)?\/\//.test(links[i].getAttribute('href'))) {
      links[i].target = '_blank';
    }
  }
})();
// adds .remark-code-has-line-highlighted class to <pre> parent elements
// of code chunks containing highlighted lines with class .remark-code-line-highlighted
(function(d) {
  const hlines = d.querySelectorAll('.remark-code-line-highlighted');
  const preParents = [];
  const findPreParent = function(line, p = 0) {
    if (p > 1) return null; // traverse up no further than grandparent
    const el = line.parentElement;
    return el.tagName === "PRE" ? el : findPreParent(el, ++p);
  };

  for (let line of hlines) {
    let pre = findPreParent(line);
    if (pre && !preParents.includes(pre)) preParents.push(pre);
  }
  preParents.forEach(p => p.classList.add("remark-code-has-line-highlighted"));
})(document);</script>

<script>
slideshow._releaseMath = function(el) {
  var i, text, code, codes = el.getElementsByTagName('code');
  for (i = 0; i < codes.length;) {
    code = codes[i];
    if (code.parentNode.tagName !== 'PRE' && code.childElementCount === 0) {
      text = code.textContent;
      if (/^\\\((.|\s)+\\\)$/.test(text) || /^\\\[(.|\s)+\\\]$/.test(text) ||
          /^\$\$(.|\s)+\$\$$/.test(text) ||
          /^\\begin\{([^}]+)\}(.|\s)+\\end\{[^}]+\}$/.test(text)) {
        code.outerHTML = code.innerHTML;  // remove <code></code>
        continue;
      }
    }
    i++;
  }
};
slideshow._releaseMath(document);
</script>
<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
(function () {
  var script = document.createElement('script');
  script.type = 'text/javascript';
  script.src  = 'https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML';
  if (location.protocol !== 'file:' && /^https?:/.test(script.src))
    script.src  = script.src.replace(/^https?:/, '');
  document.getElementsByTagName('head')[0].appendChild(script);
})();
</script>
  </body>
</html>
